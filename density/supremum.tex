\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\date{}

\begin{document}

\section{Training generative neural networks via Maximum Mean
Discrepancy
optimization}\label{training-generative-neural-networks-via-maximum-mean-discrepancy-optimization}

\subsection{Information}\label{information}

\begin{verbatim}
Dziugaite G K, Roy D M, Ghahramani Z. Training generative neural networks via maximum mean discrepancy optimization[J]. arXiv preprint arXiv:1505.03906, 2015.
\end{verbatim}

\subsection{Aim}\label{aim}

The authors consider training a deep neural network to generate samples
from an unknown distribution given i.i.d. data.


Formulate Problem

Given an input \(Z\) drawn from some fixed noise distribution
\(\mathcal{N}\), then to find a function \(G\), called generator, the
distribution of the output \(G(Z)\) is close to the data's distribution
\(P\).

\subsection{Work}\label{work}

\subsubsection{Learning to sample as
optimization}\label{learning-to-sample-as-optimization}

It is well known that, for any distribution \(P\) and any continuous
distribution \(\mathcal N\) on sufficiently regular space \(\mathbb X\)
and \(\mathbb W\), respectively, there is a function
\(G: \mathbb W \rightarrow \mathbb X\), such that \(G(W) \sim P\).

For a given family \(\{G_{\theta} \}\) of functions
\(\mathbb W \rightarrow \mathbb X\), we can cast the problem of learning
a generative model as an optimization

\[
\arg \min_{\theta} \delta(P, G_{\theta}(\mathcal N))
\] where \(\delta\) is some measure of discrepancy. In practice, we only
have i.i.d. samples \(X_1, X_2,...\) from \(P\), and so we optimize an
empirical estimate of $ \delta(P, G\_\{\theta\}(\mathcal N))$.

\subsubsection{Maximum Mean Discrepancy
(MMD)}\label{maximum-mean-discrepancy-mmd}

The MMD between \(P\) and \(G_{\theta}(\mathcal N)\) over
\(\mathcal H\), given by

\[
\delta_{MMD_{\mathcal H}} (P, G_{\theta}(\mathcal N)) = \sup_{f \in \mathcal F} E[f(X)] - E[f(Y)]
\] where \(X \sim P\) and \(Y \sim G_{\theta}(\mathcal N)\). Gretton et
al. \footnote{A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Scholkopf, Â¨
  and A. Smola. ``A Kernel Two-sample Test''. In: J. Mach. Learn. Res.
  13 (Mar. 2012), pp.~723--773.} shows that it can be solved in closed
form when \(\mathcal H\) is a reproducing kernel Hilbert space (RKHS).

Assume that \(\mathbb X\) is a nonempty compact metric space and
\(\mathcal F\) a class of functions
\(f: \mathbb X \rightarrow \mathbb R\). Let \(p\) and \(q\) be Borel
probability measures on \(\mathbb X\), and let \(X\) and \(Y\) be random
variables with distribution \(p\) and \(q\), respectively. The maximum
mean discrepancy (MMD) between \(p\) and \(q\) is

\[
\text{MMD}(\mathcal F, p,q) = \sup_{f \in \mathcal F} E[f(X)] - E[f(Y)]
\]

\end{document}

# Neural Random Forests

On the one hand, the many parameters of neural networks make thme a versatile and expressively rich tool for complex data modeling. However, their expressive power comes with the downside of increased **overfitting risk**, especially on small data sets. Conversely, random forests have fewer parameters to tune, but the greedy feature space separation by orthogonal hyperplanes results in typical stair or box-like decision surfaces, which may be advantageous for some data but suboptimal for other.

